import React, { useState } from 'react';
import { FaQuestionCircle, FaTimes, FaChevronRight, FaExternalLinkAlt, FaBook, FaGraduationCap, FaRobot } from 'react-icons/fa';

interface HelpTopic {
  id: string;
  title: string;
  content: string;
  subtopics?: HelpTopic[];
  references?: Reference[];
}

interface Reference {
  title: string;
  authors?: string;
  url: string;
  type: 'paper' | 'blog' | 'documentation';
  description?: string;
}

const HELP_TOPICS: HelpTopic[] = [
  {
    id: 'overview',
    title: 'In-Context Learning Visualization Tool',
    content: 'This tool helps you understand how large language models perform in-context learning - the ability to recognize and apply patterns from context without updating model weights. Watch GPT-2 medium analyze patterns in real-time as it generates text.',
    subtopics: [
      {
        id: 'overview-icl',
        title: 'What is In-Context Learning?',
        content: 'In-context learning is when a language model learns from examples in its input without changing its weights. Examples: Q&A format ("Q: 2+2? A: 4. Q: 3+3? A: 6. Q: 4+4? A:"), JSON completion ({"name": "John", "age": 25}, {"name": "Sarah", "age": 30}, {"name": "Mike", "age":), or code patterns (def add(a,b): return a+b; def sub(a,b): return a-b; def mul(a,b):). This tool shows HOW this learning happens through attention mechanisms.'
      },
      {
        id: 'overview-mechanism',
        title: 'The Induction Head Mechanism',
        content: 'Research shows that in-context learning primarily works through "induction heads" - attention mechanisms that detect patterns like [A][B]...[A] and predict [B]. When the model sees "cat" followed by "sat", then sees "cat" again, induction heads attend back to "sat" to predict it as the next word.'
      },
      {
        id: 'overview-learning',
        title: 'What You Will Learn',
        content: 'Understand how transformers implement few-shot learning, see which attention heads specialize in pattern recognition, observe how attention flows between tokens, and identify what makes some prompts more effective for in-context learning than others.'
      }
    ],
    references: [
      {
        title: 'In-context Learning and Induction Heads',
        authors: 'Olsson, C., Elhage, N., Nanda, N., et al.',
        url: 'https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html',
        type: 'paper',
        description: 'Foundational Anthropic research establishing induction heads as the primary mechanism for ICL'
      },
      {
        title: 'Understanding In-Context Learning',
        authors: 'Stanford AI Blog',
        url: 'https://ai.stanford.edu/blog/understanding-incontext/',
        type: 'blog',
        description: 'Accessible explanation of ICL mechanisms and their implications'
      },
      {
        title: 'Mathematical Analysis of In-Context Learning',
        authors: 'Wang, Y., et al.',
        url: 'https://arxiv.org/abs/2410.11474',
        type: 'paper',
        description: 'Source of the induction score formula IHâ‚‚(X_L) implemented in this tool'
      }
    ]
  },
  {
    id: 'icl-concepts',
    title: 'Core In-Context Learning Concepts',
    content: 'Understanding these fundamental concepts is essential for interpreting what you see in the visualizations. These are the building blocks of how transformers perform few-shot learning.',
    subtopics: [
      {
        id: 'concept-induction-heads',
        title: 'Induction Heads: The Pattern Recognition Mechanism',
        content: 'Induction heads are specific attention heads that implement pattern copying. Examples: ["function"]["add"] ... ["function"] â†’ predict "add"; ["{"]["name"] ... ["{"] â†’ predict "name"; ["Q:"]["What"] ... ["Q:"] â†’ predict "What". They detect when the current token matches a previous token that was followed by something specific. Research has shown these are the primary mechanism enabling in-context learning in transformers.'
      },
      {
        id: 'concept-copying-vs-induction',
        title: 'Copying vs. Pattern Matching',
        content: 'Copying behavior: exact repetition ("Paris is the capital of France. London is the capital of England. Paris is the capital of France."). Pattern matching: structural generalization ("English: hello, French: bonjour. English: goodbye, French: au revoir. English: thank you, French: [merci]"). Advanced induction: JSON completion ({"users": [{"id": 1, "name": "Alice"}, {"id": 2, "name": "Bob"}, {"id": 3, "name": ["Charlie"]}]). Induction heads enable both - copying when patterns are exact, generalization when patterns are structural.'
      },
      {
        id: 'concept-attention-patterns',
        title: 'Attention Patterns in ICL',
        content: 'Attention determines which previous tokens influence the current prediction. ICL examples: Code completion (def calculate(x): return x*2; def process(y): return y*2; def transform(z): â†’ attends to "return z*"), API patterns (GET /users/1 â†’ {id: 1}; GET /users/2 â†’ {id: 2}; GET /users/3 â†’ attends to "{id: 3}"), Translation (en: cat, fr: chat; en: dog, fr: chien; en: bird, fr: â†’ attends to "oiseau"). This attention flow enables pattern transfer between similar contexts.'
      },
      {
        id: 'concept-sequence-analysis',
        title: 'Token-by-Token Analysis',
        content: 'ICL happens incrementally as the model processes each token. Early tokens establish context, middle tokens form patterns, later tokens benefit from learned patterns. The tool shows this progression: watch how induction scores change as more context becomes available and patterns become clearer.'
      },
      {
        id: 'concept-mathematical-foundation',
        title: 'Mathematical Foundation',
        content: 'The tool implements the induction score formula: IHâ‚‚(X_L) = Î£ attention_weights where X_L is the current token and we sum attention from current position to all positions that complete detected [A][B]...[A] patterns. Higher scores indicate stronger reliance on pattern-based prediction rather than random guessing.'
      },
    ],
    references: [
      {
        title: 'In-context Learning and Induction Heads',
        authors: 'Olsson, C., Elhage, N., Nanda, N., et al.',
        url: 'https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html',
        type: 'paper',
        description: 'Comprehensive analysis of induction head mechanisms and their role in ICL'
      },
      {
        title: 'Mathematical Analysis of In-Context Learning',
        authors: 'Wang, Y., et al.',
        url: 'https://arxiv.org/abs/2410.11474',
        type: 'paper',
        description: 'Mathematical framework for induction score calculation used in this tool'
      },
      {
        title: 'Transformer Circuits Framework',
        authors: 'Elhage, N., et al.',
        url: 'https://transformer-circuits.pub/2021/framework/index.html',
        type: 'paper',
        description: 'Mathematical framework for understanding transformer attention mechanisms'
      }
    ]
  },
  {
    id: 'measurements',
    title: 'Understanding the Measurements',
    content: 'The tool provides quantitative metrics to measure in-context learning performance. These metrics help you understand when and how strongly ICL is occurring.',
    subtopics: [
      {
        id: 'measure-induction-score',
        title: 'Induction Score: Measuring Pattern-Based Prediction',
        content: 'This is the primary ICL metric. It quantifies how much the model relies on induction patterns versus random guessing. Score calculation: sum of attention weights from current position to all tokens that complete [A][B]...[A] patterns. Values 0-5: weak ICL, 5-20: moderate ICL, 20+: strong ICL with clear pattern recognition.'
      },
      {
        id: 'measure-induction-heads',
        title: 'Active Induction Heads: Specialization Analysis',
        content: 'Counts how many attention heads are actively performing induction at each step. More active heads indicates more robust and distributed pattern recognition. Helps identify whether ICL relies on few specialized heads or many generalized ones. Critical for understanding model reliability and failure modes.'
      },
      {
        id: 'measure-copying-score',
        title: 'Copying Score: Direct Repetition vs. Generalization',
        content: 'Measures verbatim copying behavior - when the model exactly repeats previous sequences. Helps distinguish between two ICL modes: mechanical copying ("the cat sat on the mat. the cat sat on the mat") versus intelligent generalization (learning Q&A format to answer new questions). Balance between copying and induction reveals learning sophistication.'
      },
      {
        id: 'measure-attention-analysis',
        title: 'Attention Flow Analysis: Information Routing',
        content: 'Analyzes how attention is distributed across the sequence. Entropy measures attention focus (low entropy = focused, high entropy = scattered). Sparsity indicates selective attention. These metrics reveal whether ICL involves targeted information retrieval or diffuse context integration - crucial for understanding ICL efficiency.'
      },
    ]
  },
  {
    id: 'visualization-tabs',
    title: 'Analysis Views: Six Perspectives on ICL',
    content: 'Each visualization tab provides a different analytical lens for understanding in-context learning. Use these views together to build a complete picture of how the model processes patterns.',
    subtopics: [
      {
        id: 'tab-induction-timeline',
        title: 'ðŸ“ˆ Induction Timeline: Tracking Learning Dynamics',
        content: 'Shows ICL strength evolution during generation. Blue line = induction score (pattern-based prediction strength), pink line = number of active induction heads. Spikes indicate moments when the model discovers or applies patterns. Use this to identify when ICL occurs and measure its intensity. Essential for understanding ICL temporal dynamics.'
      },
      {
        id: 'tab-attention-heatmap',
        title: 'ðŸ”¥ Attention Heatmap: Visualizing Information Flow',
        content: 'Matrix visualization of token-to-token attention weights. Bright cells show strong attention connections. Look for patterns: diagonal attention (local context), off-diagonal patterns (long-range dependencies), block structures (repeated patterns). Critical for understanding which tokens influence current predictions and how induction heads route information.'
      },
      {
        id: 'tab-attention-graph',
        title: 'ðŸ•¸ï¸ Attention Graph: Network Analysis of Context',
        content: 'Network representation where nodes are tokens and edges are attention weights. Reveals attention topology: which tokens are attention "hubs" receiving many connections, which create "bridges" between distant concepts. Helps identify key tokens that enable ICL and understand attention flow patterns critical for pattern transfer.'
      },
      {
        id: 'tab-strategy-timeline',
        title: 'ðŸ“Š Strategy Timeline: ICL Mechanism Analysis',
        content: 'Compares three ICL strategies over time: induction (pattern-based), copying (exact repetition), previous-token attention (local context). Shows how the model balances different approaches. High induction indicates sophisticated pattern recognition; high copying suggests mechanical repetition. Use to understand ICL sophistication level.'
      },
      {
        id: 'tab-token-importance',
        title: 'â­ Token Importance: Context Contribution Analysis',
        content: 'Quantifies each token\'s role in ICL: incoming attention (how much others attend to this token), outgoing attention (how much this token attends elsewhere), attention entropy (selectivity). Identifies which tokens provide essential context, which gather information, and which serve as pattern anchors for induction heads.'
      },
      {
        id: 'tab-induction-heads',
        title: 'ðŸ§  Induction Heads: Specialization and Distribution',
        content: 'Lists attention heads ranked by induction activity. Shows which layers and heads specialize in pattern recognition. Multiple active heads indicate robust, distributed ICL; few active heads suggest brittle, specialized ICL. Essential for understanding model architecture\'s role in ICL and identifying potential failure points.'
      }
    ]
  },
  {
    id: 'icl-guided-tour',
    title: 'ðŸŽ¬ ICL Guided Tour',
    content: 'The ICL Guided Tour is an automated demonstration system that showcases all tool features by running through them systematically. Perfect for first-time users and demonstrations.',
    subtopics: [
      {
        id: 'tour-overview',
        title: 'Tour Overview',
        content: 'The guided tour acts as your personal ICL assistant, automatically generating sample text, switching between visualization tabs, and explaining each feature in real-time. Total duration: ~18-20 seconds of fast-paced demonstration.'
      },
      {
        id: 'tour-features',
        title: 'Tour Features',
        content: 'Automated navigation between all tabs, professional spotlight highlighting, real-time explanations, interactive controls (play/pause/stop), speed adjustment (1x, 1.5x, 2x), step navigation, and mobile-responsive design.'
      },
      {
        id: 'tour-structure',
        title: 'Tour Structure',
        content: 'Welcome (0.5s) â†’ Generate Q&A Pattern (1s) â†’ Processing (0.5s) â†’ Learning Timeline (2s) â†’ Attention Heatmap (2s) â†’ Network Graph (2s) â†’ Induction Heads (2s) â†’ Token Importance (2s) â†’ Strategy Timeline (2s) â†’ Conclusion (2s)'
      },
      {
        id: 'tour-usage',
        title: 'How to Use',
        content: 'Click the "ICL Guide Demo" button (purple gradient with robot icon) in the header. The tour control panel appears in the top-right with all controls. Click "Start Tour" to begin the automated demonstration. Use pause/resume, speed controls, and step navigation as needed.'
      }
    ]
  },
  {
    id: 'getting-started',
    title: 'ICL Research Workflow',
    content: 'How to systematically analyze in-context learning using this tool. Follow this workflow to conduct meaningful ICL experiments and understand model behavior.',
    subtopics: [
      {
        id: 'start-experimental-design',
        title: 'Step 1: Design Your ICL Experiment',
        content: 'Choose prompts with clear patterns: \n\n**Programming patterns**: "function add(a,b) { return a+b; } function sub(a,b) { return a-b; } function mul(a,b) {" \n\n**Data patterns**: "{\"name\": \"Alice\", \"age\": 25}, {\"name\": \"Bob\", \"age\": 30}, {\"name\": \"Carol\", \"age\":" \n\n**Language patterns**: "English: hello â†’ Spanish: hola; English: goodbye â†’ Spanish: adiÃ³s; English: thank you â†’ Spanish:" \n\n**Math patterns**: "Input: 2+3, Output: 5; Input: 7+1, Output: 8; Input: 4+6, Output:" \n\nThe model needs 2+ pattern instances to demonstrate induction. Document your hypothesis about expected ICL behavior.'
      },
      {
        id: 'start-analysis-workflow',
        title: 'Step 2: Systematic Analysis Sequence',
        content: 'Start with Timeline tab (overall ICL strength trends) â†’ Induction Heads tab (which mechanisms are active) â†’ Heatmap tab (attention patterns) â†’ Strategy Timeline (mechanism balance). Use this sequence to build understanding from macro (overall ICL) to micro (specific mechanisms).'
      },
      {
        id: 'start-pattern-identification',
        title: 'Step 3: Interpreting ICL Patterns',
        content: 'Strong ICL indicators: induction scores >20, multiple active heads (>5), clear attention patterns in heatmap connecting similar contexts, high induction vs copying ratio. \n\n**Score ranges by pattern type**: Simple repetition (5-15), Q&A pairs (15-30), JSON completion (10-25), Code patterns (15-35), Translation (10-20), Random text (0-5). \n\n**Weak ICL signs**: Flat timeline, few active heads (<3), random attention patterns, high copying scores (>80% of total), scattered attention without clear structure. Use these benchmarks to evaluate ICL success and compare different prompt strategies.'
      },
      {
        id: 'start-comparative-analysis',
        title: 'Step 4: Comparative Analysis',
        content: 'Test multiple prompt formats: vary example count (2 vs 4 vs 6 examples), change pattern complexity (simple repetition vs abstract rules), modify context structure (structured vs unstructured). Compare induction scores and head activation patterns to understand what enables effective ICL.'
      }
    ]
  },
  {
    id: 'research-applications',
    title: 'Research Applications and Use Cases',
    content: 'This tool enables systematic study of in-context learning mechanisms. Use it to investigate ICL phenomena, validate theoretical predictions, and develop better few-shot learning strategies.',
    subtopics: [
      {
        id: 'research-mechanistic',
        title: 'Mechanistic Understanding of ICL',
        content: 'Investigate how induction heads implement pattern recognition: Which layers specialize in pattern detection? How does attention flow enable pattern transfer? What makes some patterns easier to learn than others? Use head activation patterns and attention flows to understand the computational mechanisms underlying few-shot learning.'
      },
      {
        id: 'research-prompt-engineering',
        title: 'Systematic Prompt Engineering',
        content: 'Optimize few-shot prompts using quantitative ICL metrics: \n\n**Test different structures**: \n- Explicit labels: "Example 1: input â†’ output; Example 2: input â†’ output" \n- Implicit patterns: "apple â†’ fruit; car â†’ vehicle; dog â†’" \n- Mixed formats: "Q: What is the capital of France? A: Paris\\nQ: What is the capital of Germany? A: Berlin\\nQ: What is the capital of Italy? A:" \n\n**Vary complexity**: \n- Simple: "1+1=2, 2+2=4, 3+3=" \n- Structured: "{\"operation\": \"add\", \"a\": 1, \"b\": 1, \"result\": 2}, {\"operation\": \"add\", \"a\": 2, \"b\": 2, \"result\": 4}" \n- Abstract: "If sunny then beach, if rainy then home, if snowy then" \n\nUse induction scores to objectively measure prompt effectiveness and identify optimal structural features.'
      },
      {
        id: 'research-failure-analysis',
        title: 'ICL Failure Mode Analysis',
        content: 'Study when and why ICL fails: \\n\\n**Common failure patterns**: \\n- Inconsistent structure: \"Q: 2+2? A: 4. Question: What is 3+3? Answer: 6\" (mixed formats) \\n- Conflicting examples: \"big â†’ small; large â†’ tiny; huge â†’ massive\" (inconsistent relationships) \\n- Insufficient context: \"cat â†’ animal\" (single example, no pattern) \\n- Noisy patterns: \"aâ†’1, bâ†’2, xyzâ†’999, câ†’3\" (disrupted sequence) \\n\\n**Analysis approach**: Identify contexts where induction heads remain inactive (<3 active heads), analyze attention patterns during poor performance (scattered, unfocused), understand copying vs. generalization trade-offs (>70% copying indicates failure to generalize). Use these insights to predict ICL reliability and develop robust few-shot strategies.'
      }
    ]
  }
];

interface HelpSystemProps {
  isOpen: boolean;
  onClose: () => void;
}

const HelpSystem: React.FC<HelpSystemProps> = ({ isOpen, onClose }) => {
  if (!isOpen) return null;

  const getIconForReferenceType = (type: 'paper' | 'blog' | 'documentation') => {
    switch (type) {
      case 'paper': return <FaGraduationCap className="text-blue-400" />;
      case 'blog': return <FaBook className="text-green-400" />;
      case 'documentation': return <FaQuestionCircle className="text-purple-400" />;
    }
  };

  return (
    <div className="fixed inset-0 bg-black/50 z-50 flex items-center justify-center p-4">
      <div className="bg-gray-900 rounded-lg shadow-xl w-full max-w-6xl max-h-[90vh] flex flex-col">
        {/* Header */}
        <div className="flex items-center justify-between p-6 border-b border-gray-700">
          <div className="flex items-center gap-3">
            <FaBook className="text-purple-400 text-2xl" />
            <div>
              <h2 className="text-2xl font-bold text-white">In-Context Learning Research Documentation</h2>
              <p className="text-gray-400 text-sm">Comprehensive guide to ICL visualization and analysis</p>
            </div>
          </div>
          <button
            onClick={onClose}
            className="text-gray-400 hover:text-white transition-colors p-2 hover:bg-gray-800 rounded-lg"
          >
            <FaTimes size={24} />
          </button>
        </div>

        {/* Scrollable Content */}
        <div className="flex-1 overflow-y-auto p-8">
          <div className="max-w-4xl mx-auto space-y-12">
            {HELP_TOPICS.map((topic, topicIndex) => (
              <section key={topic.id} className="space-y-6">
                {/* Topic Header */}
                <div className="border-l-4 border-blue-500 pl-6">
                  <h1 className="text-3xl font-bold text-white mb-4">
                    {topicIndex + 1}. {topic.title}
                  </h1>
                  <div className="prose prose-invert prose-lg max-w-none">
                    <p className="text-gray-300 leading-relaxed text-lg">{topic.content}</p>
                  </div>
                </div>

                {/* Subtopics */}
                {topic.subtopics && topic.subtopics.length > 0 && (
                  <div className="ml-6 space-y-6">
                    {topic.subtopics.map((subtopic, subtopicIndex) => (
                      <div key={subtopic.id} className="bg-gray-800/30 border border-gray-700 rounded-lg p-6">
                        <h3 className="text-xl font-semibold text-white mb-3 flex items-center gap-3">
                          <span className="w-8 h-8 bg-blue-600 rounded-full flex items-center justify-center text-sm font-bold">
                            {topicIndex + 1}.{subtopicIndex + 1}
                          </span>
                          {subtopic.title}
                        </h3>
                        <div className="prose prose-invert max-w-none">
                          <p className="text-gray-300 leading-relaxed">{subtopic.content}</p>
                        </div>
                      </div>
                    ))}
                  </div>
                )}

                {/* References */}
                {topic.references && topic.references.length > 0 && (
                  <div className="ml-6 space-y-4">
                    <h3 className="text-xl font-semibold text-white flex items-center gap-3">
                      <FaGraduationCap className="text-blue-400" />
                      Research References
                    </h3>
                    <div className="space-y-4">
                      {topic.references.map((ref, index) => (
                        <div key={index} className="bg-gray-800/50 border border-gray-700 rounded-lg p-4 hover:border-blue-500/50 transition-colors">
                          <div className="flex items-start gap-4">
                            <div className="flex-shrink-0 p-2 bg-gray-700/50 rounded-lg">
                              {getIconForReferenceType(ref.type)}
                            </div>
                            <div className="flex-1">
                              <div className="flex items-start justify-between gap-4 mb-2">
                                <div>
                                  <h4 className="text-white font-medium mb-1">{ref.title}</h4>
                                  {ref.authors && (
                                    <p className="text-blue-300 text-sm mb-2">{ref.authors}</p>
                                  )}
                                </div>
                                <a
                                  href={ref.url}
                                  target="_blank"
                                  rel="noopener noreferrer"
                                  className="flex items-center gap-2 bg-blue-600 hover:bg-blue-500 text-white text-sm px-3 py-2 rounded-md transition-colors whitespace-nowrap"
                                >
                                  <FaExternalLinkAlt size={12} />
                                  {ref.type === 'paper' ? 'Read Paper' : ref.type === 'blog' ? 'Read Article' : 'View Docs'}
                                </a>
                              </div>
                              {ref.description && (
                                <p className="text-gray-300 text-sm leading-relaxed">{ref.description}</p>
                              )}
                            </div>
                          </div>
                        </div>
                      ))}
                    </div>
                  </div>
                )}

                {/* Separator between topics */}
                {topicIndex < HELP_TOPICS.length - 1 && (
                  <hr className="border-gray-700 my-8" />
                )}
              </section>
            ))}
          </div>
        </div>
      </div>
    </div>
  );
};

export default HelpSystem;
